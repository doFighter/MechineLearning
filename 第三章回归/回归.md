# 回归

## 1 简介

回归分析是一种数学模型。当因变量和自变量具有线性关系时，它们之间便存在这样一种数学模型。

回归这个词最早是被高尔顿提出的，高尔顿受表兄达尔文的影响，非常痴迷于进化论说，因此一直希望把进化论的理论应用到实证中，证明不同人为什么会具有不同的特性。
高尔顿最著名的发现之一是发现了父亲的身高和儿子的身高之间存在着某种特定关系，他通过进一步的研究发现：子辈的平均身高是其父辈平均身高以及他们所处族群平均身高的加权平均和。

总结来说：回归是研究因变量对自变量依赖关系的一种统计分析方法，目的是通过自变量的给定值来估计或预测因变量的均值。它可用于预测、时间序列建模以及发现各种变量之间的因果关系。

回归分析具有以下特点：

1) 能够指示自变量和因变量之间的显著关系；

2) 能够指示多个自变量对一个因变量的影响强度。

回归分析还可以用于比较那些通过不同计量测得的变量之间的相互影响，如价格变动与促销活动数量之间的联系。这些益处有利于市场研究人员，数据分析人员以及数据科学家排除和衡量出一组最佳的变量，用以构建预测模型。

回归主要的种类有：

- 线性回归
- 非线性回归
- 逻辑回归
  - 二元逻辑回归
  - 多元逻辑回归

## 2 回归分析的划分

回归是一个大类，在这个大类之中可以依据自变量个数的多少进行划分。例如有一个自变量的就叫做一元回归。当含有两个自变量的时候，那么就被称作二元回归。当然还可以根据该问题是线性的还是非线性的进行分类。
分类的形式多种多样，可依据具体情况而进行分类。

例如下面的方程：

$$
h(x)=\beta_0 + \beta_1x\tag{1}
$$

![ref](images/回归-1.jpg)

假设公式（1）为某个问题的回归线，则其中 $x$ 被称为自变量，$h(x)$ 被称为因变量，$\beta_0$ 是回归线的截距，即在二维坐标上 $y$ 轴的交点，$\beta_1$ 是回归线的斜率，反应了回归线的变化速率以及相关性。

## 3 相关性分析

在回归线当中，一般会进行相关性分析，并且因变量随自变量的相关性对于求解问题来说也是至关重要的。

在当前的划分中，一般有两种相关：

- 正相关
- 负相关

在多元回归中也一样，相关性的主要决定性因素在于自变量前面的系数是正数还是负数。以下面二元回归为例：

$$
h(x)=\beta_0+\beta_1x_1+\beta_2x_2\tag{2}
$$

倘若 $\beta_1$ 是正数，则我们就认为 $h(x)$ 与 $x_1$ 是正相关的，相反如果是负数，则认为 $h(x)$ 与 $x_1$ 是负相关的，同理对于 $x_2$ 来说也一样。

>那么正相关说明什么？负相关又能说明什么呢？

事实上这个问题可以结合实际来回答。假设这是一个学习成绩受娱乐和看书两种因素的一个回归模型。假设 $x_1$ 是指看书，$x_2$ 是指娱乐。同时 $\beta_1$ 是正数，$\beta_2$ 是负数。那么在这个例子中，就说明学习成绩 $h(x)$ 与看书成正相关，与娱乐成负相关，即看书时间越多，你的学习成绩就越好，娱乐时间越多，你的成绩下滑就越大。

## 4 代价函数

上面简单描述了什么是回归，回归的作用以及相关性分析，但是这些的前提都是在有回归线，已经通过相应方法获得回归线的基础上进行分析的。因此我们需要了解，如何去确定一条回归线，怎样去判断什么样的回归线是最好的。

以下图为例：

![ref](images/回归-3.jpg)

上图有三条回归线，我们要怎样去判断哪条回归线更符合实际情况，更适合作为该问题的回归模型！

这时候就需要使用代价函数，代价函数定义多种多样，但是一般使用均方差的形式，即如公式（3）所示：

$$
E=\frac{1}{\Delta}\sum_{i=1}^N(y_i-h(x_i))^2\tag{3}
$$

为了求解的便利，其中 $\Delta$ 一般取值为 $2N$。$y_i$ 指 $x_i$ 位置的真实值，$h(x_i)$指回归线上 $x_i$ 预测值，当所有的真实值与预测值的均方差达到最小时，则说明该回归线代价最低，即拟合度最好。如下图所示：

![ref](images/回归-2.jpg)
