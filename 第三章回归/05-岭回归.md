# 岭回归

## 1 岭回归介绍

岭回归(ridge regression, Tikhonov regularization)是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。

岭回归是对数据模型使用 $L_2$ 正则化公式而来的：

$$
J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^i)-y^i)^2+\lambda\sum_{j=1}^n\theta_j^2]\tag{1}
$$

最终岭回归计算公式为：

$$
w=(X^TX+\lambda I)^{-1}X^TY\tag{2}
$$

其中 $\lambda$ 为岭系数，$I$ 为单位矩阵！

岭回归与梯度下降算法，标准方程法一样，是一种求解最优化的计算方法，从理论上更类似于标准方程法，它可以解决标准方程解决不了的不可逆矩阵问题，具有更加广泛的应用！

- $\lambda$ 参数的一般选择原则

选择λ值，使到

1）各回归系数癿岭估计基本稳定；

2）用最小二乘估计时符号不合理癿回归系数，其岭估计的符号变得合理；

3）回归系数没有不合乎实际意义癿值；

4）残差平方和增大不太多。 一般λ越大，系数β会出现稳定的假象，但是残差平方和也会更大。

取λ的方法比较多，但是结果差异较大。这是岭回归的弱点之一。

## 2 代码编写

岭回归的代码编写和标准方程法基本一致，只不过计算公式是使用（1）式，同时，多了一个参数 $\lambda$。岭回归代码如下：

```py
def RidgeRegression(X, Y, lamda):
    # 给数据增加一列，其值全为1，位置在第一列
    X = np.c_[np.ones(X.shape[0]).T, X]
    I = np.eye(X.shape[1])
    Theta = np.linalg.inv(np.dot(X.T, X) + lamda * I).dot(X.T).dot(Y)
    return Theta
```

完整代码详见 $code$ 文件夹！
